1
00:00:00,000 --> 00:00:04,665
Now, we move on to transfer learning and automated ML.

2
00:00:04,665 --> 00:00:06,599
These are two disciplines within

3
00:00:06,599 --> 00:00:09,570
machine learning that have helped push the ML field further,

4
00:00:09,570 --> 00:00:13,065
making machine learning more accessible to a larger audience.

5
00:00:13,064 --> 00:00:17,369
Transfer learning is when we use the knowledge from an already trained network,

6
00:00:17,370 --> 00:00:19,800
with a similar use case which is stored in

7
00:00:19,800 --> 00:00:22,545
the architecture of the network as well as the weights,

8
00:00:22,545 --> 00:00:24,929
and adapt that to a new use case.

9
00:00:24,929 --> 00:00:28,364
Generally that requires different classes.

10
00:00:28,364 --> 00:00:32,414
This allows us to develop models with significantly less data

11
00:00:32,414 --> 00:00:34,199
because the neural network can retain

12
00:00:34,200 --> 00:00:36,630
the knowledge it learned from previous training data,

13
00:00:36,630 --> 00:00:39,000
sometimes millions of data points,

14
00:00:39,000 --> 00:00:42,405
and slightly tunes parameter to fit our needs.

15
00:00:42,405 --> 00:00:46,880
Automated ML takes this one step further by allowing us to build

16
00:00:46,880 --> 00:00:52,030
relatively robust and scalable models without much Machine Learning experience.

17
00:00:52,030 --> 00:00:53,835
In the following project,

18
00:00:53,835 --> 00:00:56,000
you will build the Machine Learning model through

19
00:00:56,000 --> 00:00:58,159
an automated ML service provider to get

20
00:00:58,159 --> 00:01:01,359
some real hands on experience with model-building.

21
00:01:01,359 --> 00:01:03,134
In order to do this,

22
00:01:03,134 --> 00:01:06,424
we take the first N layers of the pre-trained network.

23
00:01:06,424 --> 00:01:11,090
The number N is generally all except the last or last few layers,

24
00:01:11,090 --> 00:01:13,984
depending on how deep the original network is.

25
00:01:13,984 --> 00:01:18,215
Then, we copy these layers into a new model.

26
00:01:18,215 --> 00:01:20,780
These copied layers will retain

27
00:01:20,780 --> 00:01:25,489
the same architecture as well as the same weights and activation functions.

28
00:01:25,489 --> 00:01:30,935
Then, we will attach new layers to the end to replace the old ones.

29
00:01:30,935 --> 00:01:33,365
Here, you can see the number of

30
00:01:33,364 --> 00:01:37,640
output nodes is actually different between the two models.

31
00:01:37,640 --> 00:01:41,644
This is because in the first model we are supporting four classes.

32
00:01:41,644 --> 00:01:45,155
While in the new model we only required two.

33
00:01:45,155 --> 00:01:47,375
Once we have the new network,

34
00:01:47,375 --> 00:01:50,840
we go ahead and retrain it to tune the last layers of

35
00:01:50,840 --> 00:01:55,719
the network that will give us the supported classes we're looking for.

36
00:01:55,719 --> 00:02:01,989
In this example, we copy the weights from the first N layers into the second model,

37
00:02:01,989 --> 00:02:05,539
and only retrain the last layer of the new model.

38
00:02:05,540 --> 00:02:08,480
This is specifically called transfer learning,

39
00:02:08,479 --> 00:02:11,629
where the weights on the earlier layers do not change.

40
00:02:11,629 --> 00:02:14,960
Another version of this is called fine tuning,

41
00:02:14,960 --> 00:02:19,955
where you use the same process but also retrain the earlier layers.

42
00:02:19,955 --> 00:02:22,969
Each model has its pros and cons,

43
00:02:22,969 --> 00:02:26,780
and will be largely dependent on the training data that is available.

44
00:02:26,780 --> 00:02:28,460
But at a high level,

45
00:02:28,460 --> 00:02:32,765
the idea behind transfer learning and fine tuning is that we can leverage knowledge

46
00:02:32,764 --> 00:02:34,609
that was previously learned and make

47
00:02:34,610 --> 00:02:37,900
slight modifications to accomplish in different task.

48
00:02:37,900 --> 00:02:41,015
This will generally reduce the amount of trained data

49
00:02:41,014 --> 00:02:44,299
necessary to accomplish a particular accuracy,

50
00:02:44,300 --> 00:02:46,219
which makes creating models using

51
00:02:46,219 --> 00:02:49,745
transfer learning and fine tuning much quicker and cheaper.

52
00:02:49,745 --> 00:02:54,844
Today, we see a lot of these pre-trained models available for free online.

53
00:02:54,844 --> 00:02:57,079
By using these pre-trained models,

54
00:02:57,079 --> 00:02:59,745
some of which have been trained on millions of data points,

55
00:02:59,745 --> 00:03:03,335
we take advantage of previous training cycles and require

56
00:03:03,335 --> 00:03:09,540
much less data to adjust or tune the model to identify the desired classes.

