1
00:00:00,000 --> 00:00:03,404
To begin, let's go back to our previous example,

2
00:00:03,404 --> 00:00:06,734
of determining whether or not we should go to a co-workers party?

3
00:00:06,735 --> 00:00:09,480
This is a network with a single mode.

4
00:00:09,480 --> 00:00:10,993
We had three inputs,

5
00:00:10,993 --> 00:00:12,974
along with a weight for each input,

6
00:00:12,974 --> 00:00:15,044
to show which ones were most important.

7
00:00:15,044 --> 00:00:18,329
Indeed we found that adding these weights is critical,

8
00:00:18,329 --> 00:00:23,159
because they allow us to adjust the likelihood of a certain decision based on inputs.

9
00:00:23,160 --> 00:00:27,509
In this example, it meant that we absolutely cannot attend the party

10
00:00:27,510 --> 00:00:30,300
if we are not free, which is very sensible.

11
00:00:30,300 --> 00:00:33,660
The other factor that allowed us to come to this conclusion,

12
00:00:33,659 --> 00:00:35,789
was the decision threshold.

13
00:00:35,789 --> 00:00:39,530
Had we said a decision threshold of three instead of seven,

14
00:00:39,530 --> 00:00:41,270
then indeed there would be a case,

15
00:00:41,270 --> 00:00:44,885
when we would choose to attend the party even if we aren't free.

16
00:00:44,884 --> 00:00:49,414
Now, it is clear in this example that we want a threshold above four,

17
00:00:49,414 --> 00:00:51,799
to handle the case that we aren't free.

18
00:00:51,799 --> 00:00:54,739
However, you can imagine if we're looking at

19
00:00:54,740 --> 00:00:57,690
pixels in an image or large documents of text,

20
00:00:57,689 --> 00:01:01,414
that this decision threshold may not be so easy to determine.

21
00:01:01,414 --> 00:01:05,299
This is where we see the application of activation functions.

22
00:01:05,299 --> 00:01:09,649
Activation functions, are functions that we can use as decision boundaries,

23
00:01:09,650 --> 00:01:13,719
to tell us whether or not to send a signal to subsequent layers in the network.

24
00:01:13,719 --> 00:01:17,329
Furthermore, activation functions allow us to pass a range of

25
00:01:17,329 --> 00:01:22,775
values to the following layer instead of a strict zero or one for yes or no.

26
00:01:22,775 --> 00:01:25,910
As I mentioned, activation functions serve

27
00:01:25,909 --> 00:01:28,549
as individual nodes decision boundary,

28
00:01:28,549 --> 00:01:32,015
that will determine the signal that is passed to the subsequent layer.

29
00:01:32,015 --> 00:01:34,320
An ideal activation function,

30
00:01:34,319 --> 00:01:37,519
would return a range of values which will allow us to scale

31
00:01:37,519 --> 00:01:41,509
the nodes output rather than having a binary yes or no decision.

32
00:01:41,510 --> 00:01:43,790
Starting with these activation functions

33
00:01:43,790 --> 00:01:46,310
we will piece together the different parameters,

34
00:01:46,310 --> 00:01:49,100
that are tuned when training a neural network.

35
00:01:49,099 --> 00:01:52,280
To illustrate this binary decision boundary,

36
00:01:52,280 --> 00:01:55,325
a basic threshold is in essence a step function.

37
00:01:55,325 --> 00:01:59,555
This means that until we pass our threshold we do not pass a signal,

38
00:01:59,555 --> 00:02:02,120
i.e we have a zero or no decision,

39
00:02:02,120 --> 00:02:04,250
and once we reached the threshold,

40
00:02:04,250 --> 00:02:05,780
we immediately jumped to one,

41
00:02:05,780 --> 00:02:08,134
i.e an absolute yes.

42
00:02:08,134 --> 00:02:12,995
Now in this case of attending a party a yes or no decision is fairly obvious.

43
00:02:12,995 --> 00:02:17,719
However, when executing a complex tasks such as transcribing a document,

44
00:02:17,719 --> 00:02:20,270
we want to allow for some uncertainty so

45
00:02:20,270 --> 00:02:23,270
that even if the machines transcription is not perfect,

46
00:02:23,270 --> 00:02:25,939
we can still get a decent output after which

47
00:02:25,939 --> 00:02:29,454
we can have a human make minor adjustments if necessary.

48
00:02:29,455 --> 00:02:31,690
Just like shipping products,

49
00:02:31,689 --> 00:02:34,020
allowing machines to make decisions,

50
00:02:34,020 --> 00:02:36,680
means we have to allow them to be imperfect.

51
00:02:36,680 --> 00:02:39,319
This means that we need a better decision boundary

52
00:02:39,319 --> 00:02:41,495
that will give us room for error.

53
00:02:41,495 --> 00:02:45,230
One such function is called a sigmoid function.

54
00:02:45,229 --> 00:02:50,554
Using a sigmoid function we receive an output with a range of zero to one.

55
00:02:50,555 --> 00:02:53,569
And this provides a continuous decision boundary

56
00:02:53,569 --> 00:02:56,750
where each node in the network acts as a sort of knob.

57
00:02:56,750 --> 00:02:59,870
As the network is trained these knobs are adjusted

58
00:02:59,870 --> 00:03:01,849
to produce the optimal output.

59
00:03:01,849 --> 00:03:05,164
So the way to think about this is that when training the network

60
00:03:05,164 --> 00:03:08,659
we will adjust parameters on each node in the network.

61
00:03:08,659 --> 00:03:12,259
To other common functions are the rayleigh function

62
00:03:12,259 --> 00:03:15,449
and the leaky rayleigh function.

63
00:03:15,449 --> 00:03:18,109
These simplify the activation function to

64
00:03:18,110 --> 00:03:22,490
a linear equation which allows for faster and less complex training.

65
00:03:22,490 --> 00:03:25,670
The general idea, is that we don't pay as much attention to

66
00:03:25,669 --> 00:03:29,899
the negative inputs which reduces the number of nodes that are activated,

67
00:03:29,900 --> 00:03:31,789
thereby simplifying the network.

68
00:03:31,789 --> 00:03:34,879
These are the most commonly used activation functions,

69
00:03:34,879 --> 00:03:36,859
and have been shown to generally performed

70
00:03:36,860 --> 00:03:39,530
the best among activation functions.

71
00:03:39,530 --> 00:03:41,590
So going back to our example,

72
00:03:41,590 --> 00:03:43,610
we now want to update our model with one of

73
00:03:43,610 --> 00:03:46,100
these dynamic activation functions to

74
00:03:46,099 --> 00:03:49,400
allow the model to provide more insightful decisions.

75
00:03:49,400 --> 00:03:52,310
We start by substituting the decision threshold

76
00:03:52,310 --> 00:03:54,835
with one of the activation functions.

77
00:03:54,835 --> 00:03:56,640
Here we'll go with the sigmoid,

78
00:03:56,639 --> 00:03:59,774
since we want a go or no go decision.

79
00:03:59,775 --> 00:04:02,900
Now we can use placeholders for the weights

80
00:04:02,900 --> 00:04:05,500
which will be set when the model is trained.

81
00:04:05,500 --> 00:04:08,705
Now we have a node that is completely configurable,

82
00:04:08,705 --> 00:04:12,094
instead of having hard set weights and a single threshold,

83
00:04:12,094 --> 00:04:16,610
the training process will now set these weights based on the labeled data.

84
00:04:16,610 --> 00:04:20,720
Given an input, you will provide the machine with the desired output,

85
00:04:20,720 --> 00:04:23,390
via your training labels in the training data.

86
00:04:23,389 --> 00:04:27,110
The machine will end incrementally update those weight parameters,

87
00:04:27,110 --> 00:04:30,650
to best optimize the model for all input output pairs.

88
00:04:30,649 --> 00:04:34,029
This is how the model learns from the training data,

89
00:04:34,029 --> 00:04:38,389
and why the training data is absolutely critical to the model creation process.

90
00:04:38,389 --> 00:04:42,639
The actual method for updating the weights is called Back Propagation.

91
00:04:42,639 --> 00:04:44,740
You don't need to go into the details,

92
00:04:44,740 --> 00:04:47,090
but just know that it is an algorithm used to

93
00:04:47,089 --> 00:04:51,064
automatically update those weights on the inputs in each node.

94
00:04:51,064 --> 00:04:53,959
Now one more thing I want you to know is that the nodes

95
00:04:53,959 --> 00:04:57,264
shown here is a specific type of node called the perceptron.

96
00:04:57,264 --> 00:05:00,754
In machine learning, we actually have many different types of nodes,

97
00:05:00,754 --> 00:05:03,439
that each of which has their own pros and cons,

98
00:05:03,439 --> 00:05:06,170
and are used for various applications.

99
00:05:06,170 --> 00:05:10,189
To give you an idea that node we saw previously is used

100
00:05:10,189 --> 00:05:13,910
in what's commonly known as a Multi-layer Perceptron which is

101
00:05:13,910 --> 00:05:17,870
a specific type of neural network.The idea is that you have a bunch of

102
00:05:17,870 --> 00:05:20,600
these perceptrons connected in layers without outputs of

103
00:05:20,600 --> 00:05:23,945
a previous layer feeding inputs to a subsequent layer.

104
00:05:23,944 --> 00:05:28,519
Another type of network is known as a Convolutional Neural Network which is

105
00:05:28,519 --> 00:05:30,829
widely used in computer vision tasks such

106
00:05:30,829 --> 00:05:33,829
as image classification and object detection.

107
00:05:33,829 --> 00:05:39,084
On the bottom here you see a LSTM or Long-Short Term Memory Network,

108
00:05:39,084 --> 00:05:43,899
which is commonly used in natural language tasks such as speech recognition.

109
00:05:43,899 --> 00:05:46,459
Again, you don't need to know the specifics

110
00:05:46,459 --> 00:05:48,529
of each of these networks but know that there are

111
00:05:48,529 --> 00:05:51,348
different types of nodes and networks that are specialized

112
00:05:51,348 --> 00:05:53,699
for various use cases.

